{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ASR RNN-T Demo Notebook\n",
        "\n",
        "## 1. Repository Setup and Installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!git clone https://github.com/rerum-nn/asr-rnn-t.git\n",
        "!cd asr-rnn-t"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%cd asr-rnn-t\n",
        "\n",
        "!pwd\n",
        "!ls -la"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CUDA available: True\n",
            "Python version: 3.10.4 | packaged by conda-forge | (main, Mar 24 2022, 17:39:04) [GCC 10.3.0]\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import sys\n",
        "import os\n",
        "\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "print(f\"Python version: {sys.version}\")\n",
        "\n",
        "CUSTOM_DATASET_URL = \"PLACEHOLDER\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Download Pre-trained Model Checkpoints"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/ubuntu/miniconda3/envs/dla/lib/python3.10/site-packages/huggingface_hub/file_download.py:982: UserWarning: `local_dir_use_symlinks` parameter is deprecated and will be ignored. The process to download files to a local folder has been updated and do not rely on symlinks anymore. You only need to pass a destination folder as`local_dir`.\n",
            "For more details, check out https://huggingface.co/docs/huggingface_hub/main/en/guides/download#download-files-to-local-folder.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloaded: saved/conformer-rnn-t-small/model_best.pth\n",
            "Downloaded: saved/conformer-rnn-t-small/config.yaml\n",
            "total 342792\n",
            "drwxrwxr-x  3 ubuntu ubuntu      4096 Oct 18 11:16 .\n",
            "drwxrwxr-x 20 ubuntu ubuntu      4096 Oct 18 11:16 ..\n",
            "drwxrwxr-x  3 ubuntu ubuntu      4096 Oct 18 11:15 .cache\n",
            "-rw-rw-r--  1 ubuntu ubuntu 116987623 Oct 17 18:59 checkpoint-epoch11.pth\n",
            "-rw-rw-r--  1 ubuntu ubuntu 116984994 Oct 17 09:34 checkpoint-epoch5.pth\n",
            "-rw-rw-r--  1 ubuntu ubuntu      4117 Oct 18 11:16 config.yaml\n",
            "-rw-rw-r--  1 ubuntu ubuntu        41 Oct 17 09:37 git_commit.txt\n",
            "-rw-rw-r--  1 ubuntu ubuntu       460 Oct 17 09:37 git_diff.patch\n",
            "-rw-rw-r--  1 ubuntu ubuntu     23180 Oct 17 18:59 info.log\n",
            "-rw-rw-r--  1 ubuntu ubuntu 116979327 Oct 18 11:16 model_best.pth\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from huggingface_hub import hf_hub_download\n",
        "\n",
        "os.makedirs(\"saved/conformer-rnn-t-small\", exist_ok=True)\n",
        "\n",
        "model_files = [\n",
        "    \"model_best.pth\",\n",
        "    \"config.yaml\"\n",
        "]\n",
        "\n",
        "for file_name in model_files:\n",
        "    file_path = hf_hub_download(\n",
        "        repo_id=\"Rerumnn/conformer-rnn-t\",\n",
        "        filename=file_name,\n",
        "        local_dir=\"saved/conformer-rnn-t-small\",\n",
        "        local_dir_use_symlinks=False\n",
        "    )\n",
        "    print(f\"Downloaded: {file_path}\")\n",
        "\n",
        "!ls -la saved/conformer-rnn-t-small/\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Sample Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset loaded with 2939 samples\n",
            "First sample: well\n",
            "Audio path: /home/ubuntu/asr-rnn-t/data/datasets/librispeech/test-other/2414/128291/2414-128291-0020.flac\n"
          ]
        }
      ],
      "source": [
        "from src.datasets.librispeech_dataset import LibrispeechDataset\n",
        "from src.text_encoder import RNNTTextEncoder\n",
        "from src.transforms import NormalizeRMS\n",
        "from torch import nn\n",
        "import torchaudio\n",
        "\n",
        "text_encoder = RNNTTextEncoder()\n",
        "dataset = LibrispeechDataset(\"test-other\", text_encoder=text_encoder, instance_transforms={\n",
        "    \"get_spectrogram\": torchaudio.transforms.MelSpectrogram(sample_rate=16000, n_fft=400, hop_length=160, n_mels=80),\n",
        "    \"audio\": NormalizeRMS()\n",
        "})\n",
        "\n",
        "print(f\"Dataset loaded with {len(dataset)} samples\")\n",
        "print(f\"First sample: {dataset[0]['text']}\")\n",
        "print(f\"Audio path: {dataset[0]['audio_path']}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Created inference dataset with 10 samples\n",
            "Inference completed successfully\n",
            "Logging git commit and patch...\n",
            "Run name: conformer-rnn-t-small-inference\n",
            "Run ID: g46rl6b6\n",
            "ConformerRNNT(\n",
            "  (conformer): Conformer(\n",
            "    (conv_subsampling): Sequential(\n",
            "      (0): Conv2d(1, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "      (1): ReLU()\n",
            "      (2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "      (3): ReLU()\n",
            "    )\n",
            "    (proj): Sequential(\n",
            "      (0): Linear(in_features=2560, out_features=256, bias=True)\n",
            "      (1): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "    (conformer_blocks): ModuleList(\n",
            "      (0-3): 4 x ConformerBlock(\n",
            "        (ff1): FeedForwardModule(\n",
            "          (ff_module): Sequential(\n",
            "            (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "            (1): Linear(in_features=256, out_features=1024, bias=True)\n",
            "            (2): SiLU()\n",
            "            (3): Dropout(p=0.1, inplace=False)\n",
            "            (4): Linear(in_features=1024, out_features=256, bias=True)\n",
            "            (5): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (mhsa): MultiHeadSelfAttentionModule(\n",
            "          (mhsa): Sequential(\n",
            "            (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "            (1): MHSAWithRelativePosEncoding(\n",
            "              (key_mat): Linear(in_features=256, out_features=256, bias=True)\n",
            "              (query_mat): Linear(in_features=256, out_features=256, bias=True)\n",
            "              (value_mat): Linear(in_features=256, out_features=256, bias=True)\n",
            "              (pos_mat): Linear(in_features=256, out_features=256, bias=False)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "              (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "            )\n",
            "            (2): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (conv): ConvolutionModule(\n",
            "          (conv_module): Sequential(\n",
            "            (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "            (1): Transpose()\n",
            "            (2): Conv1d(256, 512, kernel_size=(1,), stride=(1,))\n",
            "            (3): GLU(dim=1)\n",
            "            (4): Conv1d(256, 256, kernel_size=(31,), stride=(1,), padding=(15,), groups=256)\n",
            "            (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            (6): SiLU()\n",
            "            (7): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
            "            (8): Transpose()\n",
            "            (9): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (ff2): FeedForwardModule(\n",
            "          (ff_module): Sequential(\n",
            "            (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "            (1): Linear(in_features=256, out_features=1024, bias=True)\n",
            "            (2): SiLU()\n",
            "            (3): Dropout(p=0.1, inplace=False)\n",
            "            (4): Linear(in_features=1024, out_features=256, bias=True)\n",
            "            (5): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "    )\n",
            "    (output_projection): Linear(in_features=256, out_features=256, bias=True)\n",
            "  )\n",
            "  All parameters: 7226240\n",
            "  Trainable parameters: 7226240\n",
            "  (prediction_network): PredictionNetwork(\n",
            "    (embedding): Embedding(29, 256, padding_idx=0)\n",
            "    (input_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "    (hidden_activation): LSTM(256, 256, num_layers=2, batch_first=True, dropout=0.3)\n",
            "    (hidden_output): Linear(in_features=256, out_features=256, bias=True)\n",
            "    (output_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "  )\n",
            "  (joint_network): JointNetwork(\n",
            "    (activation): ReLU()\n",
            "    (linear): Linear(in_features=256, out_features=29, bias=True)\n",
            "  )\n",
            ")\n",
            "All parameters: 8360605\n",
            "Trainable parameters: 8360605\n",
            "Loading model weights from: saved/conformer-rnn-t-small-other/model_best.pth ...\n",
            "Step 0 | CER_(Argmax)_: 0.1838167701863354\n",
            "Step 0 | WER_(Argmax)_: 0.31999999999999995\n",
            "    test_CER_(Argmax): 0.1838167701863354\n",
            "    test_WER_(Argmax): 0.31999999999999995\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import subprocess\n",
        "from pathlib import Path\n",
        "import shutil\n",
        "import os\n",
        "\n",
        "dataset_dir = Path(\"data/datasets/librispeech_inference\")\n",
        "audio_dir = dataset_dir / \"audio\"\n",
        "transcriptions_dir = dataset_dir / \"transcriptions\"\n",
        "\n",
        "audio_dir.mkdir(parents=True, exist_ok=True)\n",
        "transcriptions_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "for i in range(min(10, len(dataset))):\n",
        "    sample = dataset[i]\n",
        "    audio_path = Path(sample['audio_path'])\n",
        "    text = sample['text']\n",
        "    \n",
        "    new_audio_path = audio_dir / f\"sample_{i:03d}.flac\"\n",
        "    \n",
        "    if new_audio_path.exists():\n",
        "        os.chmod(new_audio_path, 0o644) \n",
        "        new_audio_path.unlink()\n",
        "    \n",
        "    shutil.copy2(audio_path, new_audio_path)\n",
        "    \n",
        "    transcription_path = transcriptions_dir / f\"sample_{i:03d}.txt\"\n",
        "    with open(transcription_path, 'w') as f:\n",
        "        f.write(text)\n",
        "\n",
        "print(f\"Created inference dataset with {len(list(audio_dir.glob('*')))} samples\")\n",
        "\n",
        "inference_cmd = [\n",
        "    \"python\", \"inference.py\",\n",
        "    \"++inferencer.save_path=sample_inference\",\n",
        "    f\"++dir={dataset_dir}\",\n",
        "    \"writer=console\"\n",
        "]\n",
        "\n",
        "result = subprocess.run(inference_cmd, capture_output=True, text=True, timeout=300)\n",
        "\n",
        "if result.returncode == 0:\n",
        "    print(\"Inference completed successfully\")\n",
        "    print(result.stdout)\n",
        "else:\n",
        "    print(\"Inference failed:\")\n",
        "    print(result.stderr)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Calculate Metrics\n",
        "\n",
        "Now let's calculate the WER (Word Error Rate) and CER (Character Error Rate) metrics on our sample dataset to evaluate the model's performance.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Metrics calculation completed successfully:\n",
            "Dataset dir: data/datasets/librispeech_inference\n",
            "Found 10 ground truth transcriptions out of 10 audio files\n",
            "Warning: Extra predictions for 10 utterances\n",
            "Results:\n",
            "Number of samples: 10\n",
            "\tAverage WER: 0.3200 (32.00%)\n",
            "\tAverage CER: 0.1838 (18.38%)\n",
            "\n",
            "Individual sample results:\n",
            "Utterance ID\tWER\tCER\n",
            "----------------------------------------\n",
            "sample_000\t0.0000\t0.0000\n",
            "sample_001\t1.0000\t0.6250\n",
            "sample_002\t0.0000\t0.0000\n",
            "sample_003\t0.0000\t0.0000\n",
            "sample_004\t1.0000\t0.5714\n",
            "sample_005\t0.8000\t0.5217\n",
            "sample_006\t0.0000\t0.0000\n",
            "sample_007\t0.0000\t0.0000\n",
            "sample_008\t0.0000\t0.0000\n",
            "sample_009\t0.4000\t0.1200\n",
            "\n"
          ]
        }
      ],
      "source": [
        "metrics_cmd = [\n",
        "    \"python\", \"calc_metrics.py\",\n",
        "    \"--dataset_dir\", \"data/datasets/librispeech_inference\",\n",
        "    \"--predictions\", \"data/saved/sample_inference\",\n",
        "    \"--verbose\"\n",
        "]\n",
        "\n",
        "result = subprocess.run(metrics_cmd, capture_output=True, text=True, timeout=900)\n",
        "\n",
        "if result.returncode == 0:\n",
        "    print(\"Metrics calculation completed successfully:\")\n",
        "    print(result.stdout)\n",
        "else:\n",
        "    print(\"Metrics calculation failed:\")\n",
        "    print(result.stderr)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Custom Dataset \n",
        "\n",
        "```\n",
        "YourDataset/\n",
        "├── audio/\n",
        "│   ├── utterance1.wav  # can be .flac, .mp3, .m4a, .ogg\n",
        "│   ├── utterance2.wav\n",
        "│   └── ...\n",
        "└── transcriptions/  # ground truth\n",
        "    ├── utterance1.txt\n",
        "    ├── utterance2.txt\n",
        "    └── ...\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Current URL: PLACEHOLDER\n",
            "Downloading dataset from: PLACEHOLDER\n",
            "Error downloading/extracting dataset: Invalid URL 'PLACEHOLDER': No scheme supplied. Perhaps you meant https://PLACEHOLDER?\n"
          ]
        }
      ],
      "source": [
        "import gdown\n",
        "import zipfile\n",
        "import os\n",
        "\n",
        "print(\"Current URL:\", CUSTOM_DATASET_URL)\n",
        "\n",
        "def download_and_extract_dataset(url, extract_to=\"data/datasets/custom_dataset\"):\n",
        "    extract_path = Path(extract_to)\n",
        "    extract_path.mkdir(parents=True, exist_ok=True)\n",
        "    \n",
        "    print(f\"Downloading dataset from: {url}\")\n",
        "    zip_path = extract_path / \"dataset.zip\"\n",
        "    \n",
        "    try:\n",
        "        if \"drive.google.com\" in url:\n",
        "            file_id = url.split('/')[-2] if '/file/d/' in url else url.split('id=')[1].split('&')[0]\n",
        "            download_url = f\"https://drive.google.com/uc?id={file_id}\"\n",
        "        else:\n",
        "            download_url = url\n",
        "            \n",
        "        gdown.download(download_url, str(zip_path), quiet=False)\n",
        "        \n",
        "        print(f\"Extracting to: {extract_path}\")\n",
        "        with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "            zip_ref.extractall(extract_path)\n",
        "        \n",
        "        zip_path.unlink()\n",
        "        \n",
        "        print(f\"Dataset extracted to: {extract_path}\")\n",
        "        return extract_path\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"Error downloading/extracting dataset: {e}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "custom_dataset_path = download_and_extract_dataset(CUSTOM_DATASET_URL)\n",
        "custom_dataset_first_dir = next((item for item in os.listdir(custom_dataset_path)), None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if custom_dataset_first_dir:\n",
        "    print(f\"Running inference on custom dataset: {custom_dataset_first_dir}\")\n",
        "\n",
        "    audio_dir = custom_dataset_first_dir / \"audio\"\n",
        "    transcriptions_dir = custom_dataset_first_dir / \"transcriptions\"\n",
        "    \n",
        "    if audio_dir.exists() and transcriptions_dir.exists():\n",
        "        print(f\"Found {len(list(audio_dir.glob('*')))} audio files\")\n",
        "        print(f\"Found {len(list(transcriptions_dir.glob('*.txt')))} transcription files\")\n",
        "        \n",
        "        custom_inference_cmd = [\n",
        "            \"python\", \"inference.py\",\n",
        "            \"++inferencer.save_path=custom_inference\",\n",
        "            f\"++datasets.test.dir={custom_dataset_first_dir}\",\n",
        "            \"writer=console\"\n",
        "        ]\n",
        "        \n",
        "        result = subprocess.run(custom_inference_cmd, capture_output=True, text=True, timeout=600)\n",
        "        \n",
        "        if result.returncode == 0:\n",
        "            print(\"Custom dataset inference completed successfully\")\n",
        "            print(result.stdout)\n",
        "        else:\n",
        "            print(\"Custom dataset inference failed:\")\n",
        "            print(result.stderr)\n",
        "    else:\n",
        "        print(\"Error: Dataset structure is incorrect. Missing 'audio' or 'transcriptions' directories.\")\n",
        "        print(f\"Audio dir exists: {audio_dir.exists()}\")\n",
        "        print(f\"Transcriptions dir exists: {transcriptions_dir.exists()}\")\n",
        "else:\n",
        "    print(\"Skipping custom dataset inference - no dataset downloaded\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if custom_dataset_first_dir:\n",
        "    print(\"Calculating metrics\")\n",
        "    \n",
        "    custom_metrics_cmd = [\n",
        "        \"python\", \"calc_metrics.py\",\n",
        "        \"--dataset_dir\", str(custom_dataset_first_dir),\n",
        "        \"--predictions\", \"data/saved/custom_inference\",\n",
        "        \"--verbose\"\n",
        "    ]\n",
        "    \n",
        "    result = subprocess.run(custom_metrics_cmd, capture_output=True, text=True, timeout=120)\n",
        "    \n",
        "    if result.returncode == 0:\n",
        "        print(\"Custom dataset metrics calculation completed successfully:\")\n",
        "        print(result.stdout)\n",
        "    else:\n",
        "        print(\"Custom dataset metrics calculation failed:\")\n",
        "        print(result.stderr)\n",
        "else:\n",
        "    print(\"Skipping custom dataset metrics calculation - no dataset available\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "dla",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
